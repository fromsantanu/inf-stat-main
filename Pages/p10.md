# ğŸŒŸ **Tutorial: Resampling Methods**

### ğŸ“Œ **1. What are Resampling Methods?**

âœ… **Definition:**
Statistical techniques that **generate multiple samples from observed data** to estimate the **sampling distribution**, **standard errors**, and **confidence intervals** without relying on strong parametric assumptions.

âœ”ï¸ **Why useful?**

* When theoretical distribution is unknown
* Small sample sizes
* Complex estimators

---

## âœ¨ **2. Bootstrap**

âœ… **Definition:**
A method that **randomly samples with replacement** from the original data to create many â€œbootstrap samplesâ€ for estimating standard errors, confidence intervals, or bias.

---

### ğŸ”§ **Example: Estimating Mean Confidence Interval**

Letâ€™s calculate the **95% CI of the mean** using bootstrap:

```python
import numpy as np

# Original data
data = np.array([10, 12, 9, 11, 10])

# Bootstrap
n_iterations = 1000
n_size = len(data)
means = []

for i in range(n_iterations):
    sample = np.random.choice(data, size=n_size, replace=True)
    means.append(np.mean(sample))

# Calculate 95% CI
lower = np.percentile(means, 2.5)
upper = np.percentile(means, 97.5)

print("Bootstrap 95% CI for mean:", (lower, upper))
```

âœ… **Interpretation:**
The **confidence interval is estimated empirically** by repeated resampling.

---

### ğŸ“Œ **Key Points about Bootstrap:**

* Samples are of **same size as original data**
* **Sampling is with replacement**
* Can estimate **standard errors, bias, confidence intervals**

---

## âœ¨ **3. Jackknife**

âœ… **Definition:**
A resampling method where **each observation is left out one at a time**, and the estimator is recalculated for each reduced sample to assess variability or bias.

---

### ğŸ”§ **Example: Estimating Mean Standard Error**

```python
data = np.array([10, 12, 9, 11, 10])
n = len(data)
means = []

for i in range(n):
    jackknife_sample = np.delete(data, i)
    means.append(np.mean(jackknife_sample))

means = np.array(means)
jackknife_mean = np.mean(means)
standard_error = np.sqrt(((n - 1) / n) * np.sum((means - jackknife_mean) ** 2))

print("Jackknife mean estimate:", jackknife_mean)
print("Jackknife standard error:", standard_error)
```

âœ… **Interpretation:**
The **jackknife standard error** provides an estimate of variability by systematically leaving out each observation.

---

### ğŸ“Œ **Key Points about Jackknife:**

* **Leave-one-out resampling**
* Useful for **bias and standard error estimation**
* Simpler but less general than bootstrap

---

## âœ… **Summary Table**

| **Method**    | **How it Works**                         | **Use Case**                              |
| ------------- | ---------------------------------------- | ----------------------------------------- |
| **Bootstrap** | Resample with replacement multiple times | Estimate SEs, CIs, bias for any statistic |
| **Jackknife** | Leave-one-out samples (n subsamples)     | Estimate SEs, bias, influence diagnostics |

---

### ğŸ’¡ **When to Use?**

| **Bootstrap**                     | **Jackknife**                             |
| --------------------------------- | ----------------------------------------- |
| Any estimator, especially complex | Estimators that are **smooth and linear** |
| Large computing resources         | Faster for small datasets                 |

---


